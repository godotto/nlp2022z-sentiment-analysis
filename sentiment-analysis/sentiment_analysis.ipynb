{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerType(Enum):\n",
    "    SPACY = 1\n",
    "    BERT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "PADDING_MODE = \"same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(df: pd.DataFrame):\n",
    "    filtered_reviews = []\n",
    "    tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # tokenize reviews and filter stopwords\n",
    "    for review in df[\"reviews\"]:\n",
    "        # create document object with lingustic annotations\n",
    "        document = tokenizer(review)\n",
    "\n",
    "        # go through tokens and exclude stopwords\n",
    "        document = [token.lemma_ for token in document if (len(token.lemma_) > 1) and (token.lemma_.isalnum()) and (not token.is_stop)]\n",
    "        # document = ' '.join(document)\n",
    "        filtered_reviews.append(document)\n",
    "\n",
    "    return filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(all_tokens, vocab):\n",
    "    ids = []\n",
    "    for token_set in all_tokens:\n",
    "        ids.append([vocab[token] for token in token_set])\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, filepaths: list[str], tokenizer: TokenizerType = TokenizerType.SPACY):\n",
    "        df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.labels = self.le.fit_transform(df[\"sentiment\"]) \n",
    "        \n",
    "        if tokenizer == TokenizerType.SPACY:\n",
    "            self.reviews = tokenize_and_filter(df)\n",
    "            self.vocab = torchtext.vocab.build_vocab_from_iterator(self.reviews, specials=[PAD_TOKEN])\n",
    "            self.ids = numericalize_data(self.reviews, self.vocab)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.ids[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = AmazonReviewsDataset([\"../data-scraper/dishwashing tablet_reviews.csv\", \"../data-scraper/mug_reviews.csv\", \"../data-scraper/washing powder_reviews.csv\"])\n",
    "pad_index = all_data.vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors(batch, pad_index):\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_ids, batch_labels in batch:\n",
    "        ids.append(batch_ids)\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, padding_value=pad_index, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(all_data, [0.7, 0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, filter_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      padding=PADDING_MODE, kernel_size=filter_size, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_stack(x)\n",
    "        return x.max(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, dropout_rate: float, output_dim: int, feature_maps_num: int):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        self.embedding = (nn.Embedding(num_embeddings, embedding_dim, padding_idx))\n",
    "\n",
    "        self.conv1 = (ConvBlock(embedding_dim, feature_maps_num, 3))\n",
    "        self.conv2 = (ConvBlock(embedding_dim, feature_maps_num, 4))\n",
    "        self.conv3 = (ConvBlock(embedding_dim, feature_maps_num, 5))\n",
    "        \n",
    "        self.fc = (nn.Linear(3 * feature_maps_num, output_dim))\n",
    "        self.dropout = (nn.Dropout(dropout_rate))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "num_embeddings = len(all_data.vocab)\n",
    "embedding_dim = 50\n",
    "dropout_rate = 0.5\n",
    "output_dim = 3\n",
    "feature_maps_num = 100\n",
    "weight_decay = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = functools.partial(pad_tensors, pad_index=pad_index)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentCNN(num_embeddings, embedding_dim, pad_index, dropout_rate, output_dim, feature_maps_num)\n",
    "model.to(device=device)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), foreach=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2223b40a2274a959d67da0c1501455f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/godot/Documents/Politechnika/Magisterka/NLP/nlp2022z-sentiment-analysis/lib/python3.10/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    train_loss 0.5141205787658691\n",
      "1    val_loss 0.49164122343063354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564ffd9c9f0c4abe86ccbd5c711f6e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    train_loss 0.35927486419677734\n",
      "2    val_loss 0.457973837852478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799590a95ff946659d071831cb10c88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    train_loss 0.24405111372470856\n",
      "3    val_loss 0.43390730023384094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f158b9b2d542c7a27fe0f9a3b58b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    train_loss 0.18245826661586761\n",
      "4    val_loss 0.5356175899505615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4497031a3d464dad9a97f728a85a4b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    train_loss 0.12524528801441193\n",
      "5    val_loss 0.4773199260234833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ccab452e0d463ea1470cc6e733b286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    train_loss 0.10725338011980057\n",
      "6    val_loss 0.7081089019775391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d224b280c74d1a950b3d7a90e38cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    train_loss 0.09060462564229965\n",
      "7    val_loss 0.5426703691482544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1f842292943898fa92de39ed998b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 8:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    train_loss 0.06071317940950394\n",
      "8    val_loss 0.6076631546020508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c108e8c86944dcaa3f9dafb3469b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 9:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    train_loss 0.043578874319791794\n",
      "9    val_loss 0.5838634371757507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaff10333c3439983cec35801475a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 10:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    train_loss 0.028925154358148575\n",
      "10    val_loss 0.6130650639533997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fca7d1a3224c5d898504b39ca5d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 11:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11    train_loss 0.027547722682356834\n",
      "11    val_loss 0.6791728138923645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a623ac59fc084edbafc632dbcad420d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 12:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    train_loss 0.014116846956312656\n",
      "12    val_loss 0.6621667742729187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd2e304c6f841c89950b42d10b33194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 13:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13    train_loss 0.019220616668462753\n",
      "13    val_loss 0.6305027604103088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9dcaa02ae54163a014179b2bfe7e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 14:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14    train_loss 0.013695638626813889\n",
      "14    val_loss 0.7809904217720032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa35894a346641e0b8dfa6ea677cff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 15:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15    train_loss 0.014398481696844101\n",
      "15    val_loss 0.8242156505584717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e13948d917a48b696e353c75fbbe525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 16:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16    train_loss 0.013161089271306992\n",
      "16    val_loss 0.7498230934143066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e346e467b0940089f172b02288d0e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 17:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17    train_loss 0.0123518668115139\n",
      "17    val_loss 0.7850815057754517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33caf90416954a5bac174d372aea854c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 18:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18    train_loss 0.009659448638558388\n",
      "18    val_loss 0.7496472597122192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2234d0c1ad194f488b2d92750a8c388a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 19:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19    train_loss 0.005958361551165581\n",
      "19    val_loss 0.7796704769134521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674510151d264745a2658880eca6928e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 20:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    train_loss 0.004821117501705885\n",
      "20    val_loss 0.8812891840934753\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    bar = tqdm(train_loader, position=0, leave=True,\n",
    "               desc='epoch %d' % epoch)\n",
    "    for batch in bar:\n",
    "        ids, labels = batch\n",
    "\n",
    "        logits = model(ids)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss)\n",
    "\n",
    "    avg_train_loss = torch.stack(train_loss).mean()\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    print(epoch, '   train_loss', avg_train_loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for batch in val_loader:\n",
    "            ids, labels = batch\n",
    "\n",
    "            logits = model(ids)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss.append(loss)\n",
    "        avg_val_loss = torch.stack(val_loss).mean()\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "        print(epoch, '   val_loss', avg_val_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
