{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerType(Enum):\n",
    "    SPACY = 1\n",
    "    BERT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(df: pd.DataFrame):\n",
    "    filtered_reviews = []\n",
    "    # tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "    tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # tokenize reviews and filter stopwords\n",
    "    for review in df[\"reviews\"]:\n",
    "        # create document object with lingustic annotations\n",
    "        document = tokenizer(review)\n",
    "\n",
    "        # go through tokens and exclude stopwords\n",
    "        document = [token.lemma_ for token in document if (len(token.lemma_) > 1) and (token.lemma_.isalnum()) and (not token.is_stop)]\n",
    "        # document = ' '.join(document)\n",
    "        filtered_reviews.append(document)\n",
    "\n",
    "    return filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(all_tokens, vocab):\n",
    "    ids = []\n",
    "    for token_set in all_tokens:\n",
    "        ids.append([vocab[token] for token in token_set])\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, filepaths: list[str], tokenizer: TokenizerType = TokenizerType.SPACY):\n",
    "        df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.labels = self.le.fit_transform(df[\"sentiment\"]) \n",
    "        \n",
    "        if tokenizer == TokenizerType.SPACY:\n",
    "            self.reviews = tokenize_and_filter(df)\n",
    "            self.vocab = torchtext.vocab.build_vocab_from_iterator(self.reviews, specials=[PAD_TOKEN])\n",
    "            self.ids = numericalize_data(self.reviews, self.vocab)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.ids[idx]), torch.tensor(self.labels[idx], dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = AmazonReviewsDataset([\"../data-scraper/dishwashing tablet_reviews.csv\", \"../data-scraper/mug_reviews.csv\", \"../data-scraper/washing powder_reviews.csv\"])\n",
    "PAD_INDEX = all_data.vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors(batch, pad_index):\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_ids, batch_labels in batch:\n",
    "        ids.append(batch_ids)\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, padding_value=pad_index, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = functools.partial(pad_tensors, pad_index=all_data.vocab[\"<pad>\"])\n",
    "all_data_loader = DataLoader(all_data, shuffle=True, batch_size=50, collate_fn=collate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
