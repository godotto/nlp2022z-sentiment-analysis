{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "import spacy_fastlang\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerType(Enum):\n",
    "    SPACY = 1\n",
    "    BERT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "PADDING_MODE = \"same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(df: pd.DataFrame):\n",
    "    filtered_reviews = []\n",
    "    labels = []\n",
    "\n",
    "    tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "    tokenizer.add_pipe(\"language_detector\")\n",
    "\n",
    "    # tokenize reviews and filter stopwords\n",
    "    for i, review in enumerate(df[\"reviews\"]):\n",
    "        if isinstance(review, float):\n",
    "            continue\n",
    "\n",
    "        # create document object with lingustic annotations\n",
    "        document = tokenizer(review)\n",
    "        \n",
    "        if document._.language != \"en\" and document._.language_score >= 0.7:\n",
    "            continue\n",
    "\n",
    "        # go through tokens and exclude stopwords\n",
    "        document = [token.lemma_ for token in document if (len(token.lemma_) > 1) and (token.lemma_.isalnum()) and (not token.is_stop)]\n",
    "        filtered_reviews.append(document)\n",
    "        labels.append(df[\"sentiment\"].iloc[i])\n",
    "\n",
    "    return filtered_reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(all_tokens, vocab):\n",
    "    ids = []\n",
    "    for token_set in all_tokens:\n",
    "        ids.append([vocab[token] for token in token_set])\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, filepaths: list[str], tokenizer: TokenizerType = TokenizerType.SPACY):\n",
    "        df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        # self.labels = self.le.fit_transform(df[\"sentiment\"]) \n",
    "        \n",
    "        if tokenizer == TokenizerType.SPACY:\n",
    "            self.reviews, self.labels = tokenize_and_filter(df)\n",
    "            self.labels = self.le.fit_transform(self.labels)\n",
    "            self.vocab = torchtext.vocab.build_vocab_from_iterator(self.reviews, specials=[PAD_TOKEN])\n",
    "            self.ids = numericalize_data(self.reviews, self.vocab)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.ids[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "all_data = AmazonReviewsDataset([\"../data-scraper/dishwashing tablet_reviews.csv\", \"../data-scraper/mug_reviews.csv\", \"../data-scraper/washing powder_reviews.csv\"])\n",
    "pad_index = all_data.vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors(batch, pad_index):\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_ids, batch_labels in batch:\n",
    "        ids.append(batch_ids)\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, padding_value=pad_index, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(all_data, [0.7, 0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, filter_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      padding=PADDING_MODE, kernel_size=filter_size, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_stack(x)\n",
    "        return x.max(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, dropout_rate: float, output_dim: int, feature_maps_num: int):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        self.embedding = (nn.Embedding(num_embeddings, embedding_dim, padding_idx))\n",
    "\n",
    "        self.conv1 = (ConvBlock(embedding_dim, feature_maps_num, 3))\n",
    "        self.conv2 = (ConvBlock(embedding_dim, feature_maps_num, 4))\n",
    "        self.conv3 = (ConvBlock(embedding_dim, feature_maps_num, 5))\n",
    "        \n",
    "        self.fc = (nn.Linear(3 * feature_maps_num, output_dim))\n",
    "        self.dropout = (nn.Dropout(dropout_rate))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "num_embeddings = len(all_data.vocab)\n",
    "embedding_dim = 100\n",
    "dropout_rate = 0.5\n",
    "output_dim = 3\n",
    "feature_maps_num = 100\n",
    "weight_decay = 1e-3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = functools.partial(pad_tensors, pad_index=pad_index)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentCNN(num_embeddings, embedding_dim, pad_index, dropout_rate, output_dim, feature_maps_num)\n",
    "model.to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29102f7a2c6f4dd1a7c1ae0fb1731237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/78 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6037d2d93304767a528daed37328038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/17 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_bar = tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "val_bar = tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    train_bar.reset()\n",
    "    val_bar.reset()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        ids, labels = batch\n",
    "\n",
    "        logits = model(ids)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        train_bar.set_postfix(epoch=epoch, loss=loss.item())\n",
    "        train_bar.update()\n",
    "\n",
    "    avg_train_loss = torch.stack(train_loss).mean()\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_loader:\n",
    "            ids, labels = batch\n",
    "\n",
    "            logits = model(ids)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss.append(loss)\n",
    "\n",
    "            val_bar.set_postfix(epoch=epoch, loss=loss.item())\n",
    "            val_bar.update()\n",
    "\n",
    "        avg_val_loss = torch.stack(val_loss).mean()\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
