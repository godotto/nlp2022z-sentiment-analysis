{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerType(Enum):\n",
    "    SPACY = 1\n",
    "    BERT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(df: pd.DataFrame):\n",
    "    filtered_reviews = []\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "    # tokenize reviews and filter stopwords\n",
    "    for review in df[\"reviews\"]:\n",
    "        # create document object with lingustic annotations\n",
    "        document = tokenizer(abstract)\n",
    "\n",
    "        # go through tokens and exclude stopwords\n",
    "        document = [token.lemma_ for token in document if (len(token.lemma_) > 1) and (token.lemma_.isalnum()) and (not token.is_stop)]\n",
    "        document = ' '.join(document)\n",
    "        filtered_reviews.append(document)\n",
    "\n",
    "    return filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(example, vocab):\n",
    "    ids = [vocab[token] for token in example['tokens']]\n",
    "    return {'ids': ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, filepaths, tokenizer: TokenizerType = TokenizerType.SPACY):\n",
    "        df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.labels = self.le.fit_transform(df[\"sentiment\"].unique())\n",
    "\n",
    "        # self.tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", language=\"en_core_web_sm\") if tokenizer == \"spacy\" else BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        if tokenizer == TokenizerType.SPACY:\n",
    "            filtered_reviews = tokenize_and_filter(df)\n",
    "            torchtext.vocab.build_vocab_from_iterator(filtered_reviews)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
